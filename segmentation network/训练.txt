singleconv_doubleres_bottlenet_cbma  在单个转基因数据集上训练
卷积用的single conv 用了CBAM attention 用了两层残差

网络结构：

class MINet32_sigle_conv_2res(nn.Module):
   
    def __init__(self, nchannels=1, nlabels=1, dim=3, batchnorm=True, dropout=False):
      
        super().__init__()

        self.nchannels = nchannels
        self.nlabels = nlabels
        self.dims = dim
        self.batchnorm = batchnorm
        self.dropout = dropout
        # 3D Convolutional layers
        self.pool1 = nn.AvgPool3d(2)
        self.pool2 = nn.AvgPool3d(4)
        self.conv1 = SingleConv(nchannels,32)
        self.conv2 = ResidualBlock(32)
        self.conv22 = ResidualBlock(32)
        self.conv3 = SingleConv(nchannels, 32)
        self.conv4 = ResidualBlock(32)
        self.conv44 = ResidualBlock(32)
        self.conv5 = SingleConv(nchannels, 32)
        self.conv6 = ResidualBlock(32)
        self.conv66 = ResidualBlock(32)
        self.up1 = nn.ConvTranspose3d(32, 32, 2, stride=2)
        self.up2 = nn.ConvTranspose3d(32, 32, 2, stride=2)
        self.conv7 = SingleConv(32, 16)
        self.up3 = nn.ConvTranspose3d(16, 16, 2, stride=2)

        self.cbam = CBAMBlock(channel=80, reduction=16, kernel_size=7)

        self.conv8  = SingleConv(80, 40)
        self.conv9 = nn.Conv3d(40, nlabels, 1)


    def forward(self, x):

        x0 = x
        x1 = self.pool1(x)
        x2 = self.pool2(x)

        x0 = self.conv1(x0)
        x0 = self.conv2(x0)
        x0 = self.conv22(x0)

        x1 = self.conv3(x1)
        x1 = self.conv4(x1)
        x1 = self.conv44(x1)

        x2 = self.conv5(x2)
        x2 = self.conv6(x2)
        x2 = self.conv66(x2)

        x1 = self.up1(x1)

        x2 = self.up2(x2)
        x2 = self.conv7(x2)
        x2 = self.up3(x2)

        merge = torch.cat([x0, x1, x2], dim=1)

        merge = self.cbam(merge)

        merge = self.conv8(merge)
        merge = self.conv9(merge)
        out = nn.Sigmoid()(merge)

        return out


singleconv_doubleres_bottlenet_cbmax4  在单个转基因数据集上训练
卷积用的single conv 用了CBAM attention 用了两层残差
attention用了4个
网络结构：


class MINet32_sigle_conv_2res(nn.Module):
    """INPUT - 3DCONV - 3DCONV - 3DCONV - 3DCONV - FCN """

    def __init__(self, nchannels=1, nlabels=1, dim=3, batchnorm=True, dropout=False):
        """
        Builds the network structure with the provided parameters

        Input:
        - nchannels (int): number of input channels to the network
        - nlabels (int): number of labels to be predicted
        - dim (int): dimension of the network
        - batchnorm (boolean): sets if network should have batchnorm layers
        - dropout (boolean): set if network should have dropout layers
        """
        super().__init__()

        self.nchannels = nchannels
        self.nlabels = nlabels
        self.dims = dim
        self.batchnorm = batchnorm
        self.dropout = dropout
        # 3D Convolutional layers
        self.pool1 = nn.AvgPool3d(2)
        self.pool2 = nn.AvgPool3d(4)
        self.conv1 = SingleConv(nchannels,32)
        self.conv2 = ResidualBlock(32)
        self.conv22 = ResidualBlock(32)
        self.conv3 = SingleConv(nchannels, 32)
        self.conv4 = ResidualBlock(32)
        self.conv44 = ResidualBlock(32)
        self.conv5 = SingleConv(nchannels, 32)
        self.conv6 = ResidualBlock(32)
        self.conv66 = ResidualBlock(32)

        self.cbam1 = CBAMBlock(channel=32, reduction=16, kernel_size=7)


        self.up1 = nn.ConvTranspose3d(32, 32, 2, stride=2)

        self.cbam2 = CBAMBlock(channel=32, reduction=16, kernel_size=7)


        self.up2 = nn.ConvTranspose3d(32, 32, 2, stride=2)
        self.conv7 = SingleConv(32, 16)
        self.up3 = nn.ConvTranspose3d(16, 16, 2, stride=2)

        self.cbam3 = CBAMBlock(channel=16, reduction=16, kernel_size=7)



        self.cbam = CBAMBlock(channel=80, reduction=16, kernel_size=7)

        self.conv8  = SingleConv(80, 40)
        self.conv9 = nn.Conv3d(40, nlabels, 1)



    def forward(self, x):

        x0 = x
        x1 = self.pool1(x)
        x2 = self.pool2(x)

        x0 = self.conv1(x0)
        x0 = self.conv2(x0)
        x0 = self.conv22(x0)


        x1 = self.conv3(x1)
        x1 = self.conv4(x1)
        x1 = self.conv44(x1)

        x2 = self.conv5(x2)
        x2 = self.conv6(x2)
        x2 = self.conv66(x2)

        x1 = self.up1(x1)

        x2 = self.up2(x2)
        x2 = self.conv7(x2)
        x2 = self.up3(x2)

        x0 = self.cbam1(x0)
        x1 = self.cbam2(x1)
        x2 = self.cbam3(x2)

        merge = torch.cat([x0, x1, x2], dim=1)

        merge = self.cbam(merge)

        merge = self.conv8(merge)
        merge = self.conv9(merge)
        out = nn.Sigmoid()(merge)


        return out

逆卷积用采样，cbam放在最后 singleconv_doubleres_bottlenet__upsample_cbma_last


class MINet32_sigle_conv_2res(nn.Module):
    """INPUT - 3DCONV - 3DCONV - 3DCONV - 3DCONV - FCN """

    def __init__(self, nchannels=1, nlabels=1, dim=3, batchnorm=True, dropout=False):
        """
        Builds the network structure with the provided parameters

        Input:
        - nchannels (int): number of input channels to the network
        - nlabels (int): number of labels to be predicted
        - dim (int): dimension of the network
        - batchnorm (boolean): sets if network should have batchnorm layers
        - dropout (boolean): set if network should have dropout layers
        """
        super().__init__()

        self.nchannels = nchannels
        self.nlabels = nlabels
        self.dims = dim
        self.batchnorm = batchnorm
        self.dropout = dropout
        # 3D Convolutional layers
        self.pool1 = nn.AvgPool3d(2)
        self.pool2 = nn.AvgPool3d(4)
        self.conv1 = SingleConv(nchannels,32)
        #self.conv1 = Conv3d_dw(nchannels,32,1)
        self.conv2 = ResidualBlock(32)
        self.conv22 = ResidualBlock(32)
        self.conv3 = SingleConv(nchannels, 32)
        #self.conv3 = Conv3d_dw(nchannels, 32, 1)
        self.conv4 = ResidualBlock(32)
        self.conv44 = ResidualBlock(32)
        self.conv5 = SingleConv(nchannels, 32)
        #self.conv5 = Conv3d_dw(nchannels, 32, 1)
        self.conv6 = ResidualBlock(32)
        self.conv66 = ResidualBlock(32)




        #self.up1 = nn.ConvTranspose3d(32, 32, 2, stride=2)
        self.up1 = nn.Upsample(mode="trilinear", scale_factor=2, align_corners=False)




        #self.up2 = nn.ConvTranspose3d(32, 32, 2, stride=2)
        self.up2 = nn.Upsample(mode="trilinear", scale_factor=2, align_corners=False)
        self.conv7 = SingleConv(32, 16)
        #self.conv7 = Conv3d_dw(32, 16, 1)

        #self.up3 = nn.ConvTranspose3d(16, 16, 2, stride=2)
        self.up3 = nn.Upsample(mode="trilinear", scale_factor=2, align_corners=False)
        self.conv8  = SingleConv(80, 40)
        #self.conv8 = Conv3d_dw(80, 40, 1)

        self.cbam = CBAMBlock(channel=40, reduction=16, kernel_size=7)
        self.conv9 = nn.Conv3d(40, nlabels, 1)



    def forward(self, x):

        x0 = x
        x1 = self.pool1(x)
        x2 = self.pool2(x)

        x0 = self.conv1(x0)
        x0 = self.conv2(x0)
        x0 = self.conv22(x0)


        x1 = self.conv3(x1)
        x1 = self.conv4(x1)
        x1 = self.conv44(x1)

        x2 = self.conv5(x2)
        x2 = self.conv6(x2)
        x2 = self.conv66(x2)

        x1 = self.up1(x1)

        x2 = self.up2(x2)
        x2 = self.conv7(x2)
        x2 = self.up3(x2)

        merge = torch.cat([x0, x1, x2], dim=1)

        merge = self.conv8(merge)
        merge = self.cbam(merge)
        merge = self.conv9(merge)
        out = nn.Sigmoid()(merge)


        return out
